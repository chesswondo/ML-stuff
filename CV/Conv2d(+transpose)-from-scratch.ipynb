{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Conv2d and Conv2dTranspose"
      ],
      "metadata": {
        "id": "sireBv6tjj6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "vmPiOZEgNmEx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4nEdtocQtxx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Курсив*## 1. Conv2d"
      ],
      "metadata": {
        "id": "caRp1O9lkRZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Operation"
      ],
      "metadata": {
        "id": "rubIDo9oNcno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_filter(a_slice_prev, W, b):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    a_slice_prev -- slice of input data of shape (n_C_prev, f, f)\n",
        "    W -- Weight parameters - matrix of shape (n_C_prev, f, f)\n",
        "    b -- Bias parameters - matrix of shape (1, 1, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    \"\"\"\n",
        "\n",
        "    # Element-wise product between a_slice_prev and W\n",
        "    s = a_slice_prev * W\n",
        "\n",
        "    # Sum over all entries of the volume s\n",
        "    z = torch.sum(s)\n",
        "\n",
        "    # Add bias b to Z\n",
        "    Z = z + torch.squeeze(b)\n",
        "\n",
        "    return Z\n",
        "\n",
        "\n",
        "def conv_forward(A_prev, W, b):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer,\n",
        "        torch tensor of shape (m, n_C_prev, n_H_prev, n_W_prev)\n",
        "    W -- Weights, torch tensor of shape (n_C, n_C_prev, f, f)\n",
        "    b -- Biases, torch tensor of shape (n_C, 1, 1, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_C, n_H, n_W)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve dimensions from A_prev's shape\n",
        "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
        "\n",
        "    # Retrieve dimensions from W's shape\n",
        "    (n_C, n_C_prev, f, f) = W.shape\n",
        "\n",
        "    #default conv2d parameters, keep them just for visibility\n",
        "    stride = 1\n",
        "    pad = 0\n",
        "\n",
        "    # Compute the dimensions of the CONV output volume using the formula from pytorch documentation\n",
        "    n_H = int((n_H_prev+(2*pad)-f)/stride)+1\n",
        "    n_W = int((n_W_prev+(2*pad)-f)/stride)+1\n",
        "\n",
        "    # Initialize the output volume Z with zeros\n",
        "    Z =  torch.zeros((m, n_C, n_H, n_W), dtype=torch.float32)\n",
        "\n",
        "    # Create A_prev_copy because I need A_prev unchanged for the backprop later\n",
        "    A_prev_copy = A_prev.clone()\n",
        "\n",
        "    for i in range(m):                   # loop over the batch of training examples\n",
        "        a_prev_copy = A_prev_copy[i]     # Select ith training example\n",
        "        for h in range(n_H):             # loop over vertical axis of the output\n",
        "            # Find the vertical start and end of the current slice\n",
        "            vert_start = stride * h\n",
        "            vert_end = vert_start + f\n",
        "\n",
        "            for w in range(n_W):         # loop over horizontal axis of the output\n",
        "                # Find the horizontal start and end of the current slice\n",
        "                horiz_start = stride * w\n",
        "                horiz_end = horiz_start + f\n",
        "\n",
        "                for c in range(n_C):     # loop over channels of the output\n",
        "\n",
        "                    # Define the 3D slice of a_prev_pad\n",
        "                    a_slice_prev = a_prev_copy[:, vert_start:vert_end, horiz_start:horiz_end]\n",
        "\n",
        "                    # Apply convolution on the 3D slice with the filter W and bias b, to get back one output neuron\n",
        "                    weights = W[c, :, :, :]\n",
        "                    biases  = b[c, :, :, :]\n",
        "                    Z[i, c, h, w] = apply_filter(a_slice_prev, weights, biases)   # A function that I defined earlier\n",
        "\n",
        "\n",
        "    # Save information for the backprop\n",
        "    cache = (A_prev, W, b)\n",
        "\n",
        "    return Z, cache"
      ],
      "metadata": {
        "id": "IFzsQFxVJMfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I'll define the standart PyTorch Conv2d layer with default parameters to check its ouput and compare it with my custom one (must be very similar)"
      ],
      "metadata": {
        "id": "KwPndyuSt4Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run these calculations 10 times to be sure that errors are very small\n",
        "for i in range(10):\n",
        "\n",
        "  # define some random values that I'll use for testing\n",
        "  in_channels  = random.randint(1, 10)\n",
        "  out_channels = random.randint(1, 10)\n",
        "  kernel_size  = random.randint(1, 5)\n",
        "  m = random.randint(1, 10)\n",
        "  n_H_prev = random.randint(5, 10)  # to make it larger than kernel_size\n",
        "  n_W_prev = random.randint(5, 10)  # to make it larger than kernel_size\n",
        "\n",
        "  # define the standart PyTorch Conv2d layer\n",
        "  model = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
        "\n",
        "  # initialize weights and biases for the layer with random values\n",
        "  torch.nn.init.xavier_uniform_(model.weight)\n",
        "  torch.nn.init.uniform_(model.bias)\n",
        "\n",
        "  # initialize input with random values\n",
        "  A_prev = torch.rand(m, in_channels, n_H_prev, n_W_prev)\n",
        "\n",
        "  # create the same weights and biases to feed them into my custom function\n",
        "  W = model.weight\n",
        "  b = model.bias[:, None, None, None]  # just change the dimension\n",
        "\n",
        "  # get results from the custom function\n",
        "  Z, cache_conv = conv_forward(A_prev, W, b)\n",
        "\n",
        "  # get results from the standart PyTorch layer\n",
        "  model_out = model(A_prev)\n",
        "\n",
        "  # compare the results using mean squared error (MSE) as an appropriate metric\n",
        "  error = mean_squared_error(model_out.detach().numpy().reshape(m, -1), Z.detach().numpy().reshape(m, -1))\n",
        "\n",
        "  # print out the error. The closer it is to zero, the better\n",
        "  print(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHVwmpdl3CmW",
        "outputId": "3b138e6f-1e67-4b3d-9f43-bae9c1a30951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7658432e-15\n",
            "1.1876611e-15\n",
            "6.642834e-16\n",
            "3.3968198e-15\n",
            "7.764281e-14\n",
            "4.0212227e-16\n",
            "6.776145e-15\n",
            "1.1964136e-14\n",
            "2.39714e-15\n",
            "5.901334e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can see that error value ranges a little from ~ xe-16 to xe-14 that are very small numbers."
      ],
      "metadata": {
        "id": "5-VcsPhNyNir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward Operation"
      ],
      "metadata": {
        "id": "ScEaBJHDktKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), torch tensor of shape (m, n_C, n_H, n_W)\n",
        "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
        "               torch tensor of shape (m, n_C_prev, n_H_prev, n_W_prev)\n",
        "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
        "          torch tensor of shape (n_C, n_C_prev, f, f)\n",
        "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
        "          torch tensor of shape (n_C, 1, 1, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve information from \"cache\"\n",
        "    (A_prev, W, b) = cache\n",
        "    # Retrieve dimensions from A_prev's shape\n",
        "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
        "    # Retrieve dimensions from W's shape\n",
        "    (n_C, n_C_prev, f, f) = W.shape\n",
        "\n",
        "    #defult parameter\n",
        "    stride = 1\n",
        "\n",
        "    # Retrieve dimensions from dZ's shape\n",
        "    (m, n_C, n_H, n_W) = dZ.shape\n",
        "\n",
        "    # Initialize dA_prev, dW, db with the correct shapes\n",
        "    dA_prev = torch.zeros(A_prev.shape)\n",
        "    dW = torch.zeros(W.shape)\n",
        "    db = torch.zeros(b.shape)\n",
        "\n",
        "    # Pad A_prev and dA_prev\n",
        "    A_prev_pad = A_prev.clone()\n",
        "    dA_prev_pad = dA_prev.clone()\n",
        "\n",
        "    for i in range(m):                     # loop over the training examples\n",
        "\n",
        "        # select ith training example from A_prev_pad and dA_prev_pad\n",
        "        a_prev_pad = A_prev_pad[i]\n",
        "        da_prev_pad = dA_prev_pad[i]\n",
        "\n",
        "        for h in range(n_H):               # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):           # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):       # loop over the channels of the output volume\n",
        "\n",
        "                    # Find the corners of the current \"slice\"\n",
        "                    vert_start = stride * h\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = stride * w\n",
        "                    horiz_end = horiz_start + f\n",
        "\n",
        "                    # Use the corners to define the slice from a_prev_pad\n",
        "                    a_slice = a_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end]\n",
        "\n",
        "                    # Update gradients for the window and the filter's parameters\n",
        "                    da_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end] += W[c,:,:,:] * dZ[i, c, h, w]\n",
        "                    dW[c,:,:,:] += a_slice * dZ[i, c, h, w]\n",
        "                    db[c,:,:,:] += dZ[i, c, h, w]\n",
        "\n",
        "        # Set the ith training example's dA_prev to the unpadded da_prev_pad\n",
        "        dA_prev[i, :, :, :] = da_prev_pad[:, :, :]\n",
        "\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "WttXue8S7AfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run these calculations 10 times to be sure that errors are very small\n",
        "for i in range(10):\n",
        "\n",
        "  # define some random values that I'll use for testing\n",
        "  in_channels  = random.randint(1, 10)\n",
        "  out_channels = random.randint(1, 10)\n",
        "  kernel_size  = random.randint(1, 5)\n",
        "  m = random.randint(1, 10)\n",
        "  n_H_prev = random.randint(5, 10)  # to make it larger than kernel_size\n",
        "  n_W_prev = random.randint(5, 10)  # to make it larger than kernel_size\n",
        "\n",
        "  # define the standart PyTorch Conv2d layer\n",
        "  model = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
        "\n",
        "  # initialize weights and biases for the layer with random values\n",
        "  torch.nn.init.xavier_uniform_(model.weight)\n",
        "  torch.nn.init.uniform_(model.bias)\n",
        "\n",
        "  # initialize input with random values\n",
        "  A_prev = torch.rand(m, in_channels, n_H_prev, n_W_prev, requires_grad=True)\n",
        "\n",
        "  # create the same weights and biases to feed them into my custom function\n",
        "  W = model.weight.requires_grad_()\n",
        "  b = model.bias[:, None, None, None].requires_grad_()\n",
        "\n",
        "  # get results from the standart PyTorch layer\n",
        "  model_out = model(A_prev)\n",
        "  # to use .backward() method I need to get scalar value\n",
        "  T = torch.sum(model_out)\n",
        "  # call this method to compute gradients\n",
        "  T.backward()\n",
        "\n",
        "  # get results from the custom function (forward path)\n",
        "  Z, cache_conv = conv_forward(A_prev, W, b)\n",
        "\n",
        "  # get gradients from the custom function (use ones_like because I have dL/dy like this by default)\n",
        "  dA, dW, db = conv_backward(torch.ones_like(Z), cache_conv)\n",
        "\n",
        "  # compare the results using mean squared error (MSE) as an appropriate metric\n",
        "  dA_error = mean_squared_error(A_prev.grad.detach().numpy().reshape(m, -1), dA.detach().numpy().reshape(m, -1))\n",
        "  dW_error = mean_squared_error(model.weight.grad.detach().numpy().reshape(out_channels, -1), dW.detach().numpy().reshape(out_channels, -1))\n",
        "  db_error = mean_squared_error(model.bias.grad.detach().numpy().reshape(out_channels, -1), db.detach().numpy().reshape(out_channels, -1))\n",
        "\n",
        "  # print out the results\n",
        "  print(\"Gradient of inputs error =\", dA_error)\n",
        "  print(\"Gradient of weights error =\", dW_error)\n",
        "  print(\"Gradient of biases error =\", db_error, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oBMo15j7Aht",
        "outputId": "e9333847-ad31-48be-c534-068c0f5d6501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of inputs error = 7.438428e-15\n",
            "Gradient of weights error = 4.5474735e-13\n",
            "Gradient of biases error = 0.0 \n",
            "\n",
            "Gradient of inputs error = 4.744746e-15\n",
            "Gradient of weights error = 0.0\n",
            "Gradient of biases error = 0.0 \n",
            "\n",
            "Gradient of inputs error = 3.7579816e-14\n",
            "Gradient of weights error = 0.0\n",
            "Gradient of biases error = 0.0 \n",
            "\n",
            "Gradient of inputs error = 1.1142872e-15\n",
            "Gradient of weights error = 0.0\n",
            "Gradient of biases error = 0.0 \n",
            "\n",
            "Gradient of inputs error = 5.629224e-15\n",
            "Gradient of weights error = 0.0\n",
            "Gradient of biases error = 0.0 \n",
            "\n",
            "Gradient of inputs error = 2.3840164e-14\n",
            "Gradient of weights error = 0.0\n",
            "Gradient of biases error = 0.0 \n",
            "\n",
            "Gradient of inputs error = 4.8839744e-17\n",
            "Gradient of weights error = 4.760093e-09\n",
            "Gradient of biases error = 0.0 \n",
            "\n",
            "Gradient of inputs error = 8.5664534e-16\n",
            "Gradient of weights error = 0.0\n",
            "Gradient of biases error = 0.0 \n",
            "\n",
            "Gradient of inputs error = 2.6048804e-14\n",
            "Gradient of weights error = 0.0\n",
            "Gradient of biases error = 0.0 \n",
            "\n",
            "Gradient of inputs error = 1.7356117e-14\n",
            "Gradient of weights error = 0.0\n",
            "Gradient of biases error = 0.0 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv2dTranspose"
      ],
      "metadata": {
        "id": "pjhYb1ouk1lP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Operation"
      ],
      "metadata": {
        "id": "i6840-cLk6Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_matrix(a_prev_copy, filter, index, h, w, c, res):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "  a_prev_copy -- ith output activation of the previous layer, torch tensor of shape (n_C_prev, n_H_prev, n_W_prev)\n",
        "  filter -- weights, torch tensor of shape (n_C_prev, n_C, f, f)\n",
        "  index -- no. of an example in a batch\n",
        "  h -- verical axis coordinate\n",
        "  w -- horizontal axis coordinate\n",
        "  c -- no. of a channel\n",
        "  Z -- current state of output tensor of shape (m, n_C, n_H, n_W)\n",
        "\n",
        "  Returns:\n",
        "  res -- conv output, torch tensor of shape (m, n_C, n_H, n_W)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # get current pixel that we're gonna process\n",
        "  curr = a_prev_copy[c][h][w]\n",
        "  # the main loop over the filters (n_C)\n",
        "  for num_filter in range(filter.shape[1]):\n",
        "    # multiply curent pixel value and appropriate 2D filter\n",
        "    add_filter = curr*filter[c][num_filter]\n",
        "    # loop over the height and width of the filter\n",
        "    for i in range(add_filter.shape[0]):\n",
        "      for j in range(add_filter.shape[1]):\n",
        "        # add values to res matrix\n",
        "        res[index][num_filter][h+i][w+j] += add_filter[i][j]\n",
        "\n",
        "\n",
        "  return res\n",
        "\n",
        "\n",
        "def conv_transpose_forward(A_prev, W, b):\n",
        "    \"\"\"\n",
        "    Forward propagation for a transpose conv function\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer,\n",
        "        torch tensor of shape (m, n_C_prev, n_H_prev, n_W_prev)\n",
        "    W -- Weights, torch tensor of shape (n_C_prev, n_C, f, f)\n",
        "    b -- Biases, torch tensor of shape (n_C, 1, 1, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- conv transpose output, torch tensor of shape (m, n_C, n_H, n_W)\n",
        "    cache -- cache of values needed for the conv_transpose_backward() function\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve dimensions from A_prev's shape\n",
        "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
        "\n",
        "    # Retrieve dimensions from W's shape\n",
        "    (n_C_prev, n_C, f, f) = W.shape\n",
        "\n",
        "    # Compute the dimensions of the output\n",
        "    n_H = int(n_H_prev+f-1)\n",
        "    n_W = int(n_W_prev+f-1)\n",
        "\n",
        "    # Initialize the output Z with zeros\n",
        "    Z =  torch.zeros((m, n_C, n_H, n_W), dtype=torch.float32)\n",
        "\n",
        "    # Create A_prev_copy because I need A_prev unchanged to use it later\n",
        "    A_prev_copy = A_prev.clone()\n",
        "\n",
        "    for i in range(m):                      # loop over the batch of training examples\n",
        "        a_prev_copy = A_prev_copy[i]        # Select ith training example\n",
        "        for h in range(n_H_prev):           # loop over vertical axis of the input\n",
        "            for w in range(n_W_prev):       # loop over horizontal axis of the input\n",
        "                for c in range(n_C_prev):   # loop over channels of the input\n",
        "                    Z = add_matrix(a_prev_copy, W, i, h, w, c, Z)   #use the function I defined earlier\n",
        "\n",
        "        # add bias\n",
        "        for x in range(n_C): Z[i][x] += torch.squeeze(b[x])\n",
        "\n",
        "    # Save information in cache for the backprop\n",
        "    cache = (A_prev, W, b)\n",
        "\n",
        "    return Z, cache"
      ],
      "metadata": {
        "id": "yrGVGLRtT8O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some random values that I'll use for testing\n",
        "in_channels  = random.randint(1, 10)\n",
        "out_channels = random.randint(1, 10)\n",
        "kernel_size  = random.randint(1, 5)\n",
        "m = random.randint(1, 10)\n",
        "n_H_prev = random.randint(1, 10)\n",
        "n_W_prev = random.randint(1, 10)\n",
        "\n",
        "# define the standart PyTorch ConvTranspose2d layer\n",
        "model = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
        "\n",
        "# create the same weights and biases to feed them into my custom function\n",
        "torch.nn.init.xavier_uniform_(model.weight)\n",
        "torch.nn.init.uniform_(model.bias)\n",
        "\n",
        "# initialize input with random values\n",
        "A_prev = torch.rand(m, in_channels, n_H_prev, n_W_prev)\n",
        "\n",
        "# create the same weights and biases to feed them into my custom function\n",
        "W = model.weight\n",
        "b = model.bias[:, None, None, None]\n",
        "\n",
        "# get results from the custom function\n",
        "Z, cache_conv = conv_transpose_forward(A_prev, W, b)\n",
        "\n",
        "# get results from the standart PyTorch layer\n",
        "model_out = model(A_prev)\n",
        "\n",
        "# compare the results using mean squared error (MSE) as an appropriate metric\n",
        "error = mean_squared_error(model_out.detach().numpy().reshape(m, -1), Z.detach().numpy().reshape(m, -1))\n",
        "\n",
        "# print out the results\n",
        "print(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IXQ-n5rT8RA",
        "outputId": "ca900f88-11d1-4893-fb1d-bed0a243e113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.6761246e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I got a very small value so the functions are almost identical"
      ],
      "metadata": {
        "id": "srmi1ikjoWgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward Operation"
      ],
      "metadata": {
        "id": "KgV5yWVtk9p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_transpose_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    dZ -- gradient of the cost with respect to the output of the conv_transpose layer (Z), torch tensor of shape (m, n_C, n_H, n_W)\n",
        "    By default it contains m*n_C*n_H*n_W ones\n",
        "    cache -- cache of values, output of conv_transpose_forward()\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- gradient of the cost with respect to the input of the conv_transpose layer (A_prev),\n",
        "               torch tensor of shape (m, n_C_prev, n_H_prev, n_W_prev)\n",
        "    dW -- gradient of the cost with respect to the weights of the conv_transpose layer (W)\n",
        "          torch tensor of shape (n_C_prev, n_C, f, f)\n",
        "    db -- gradient of the cost with respect to the biases of the conv_transpose layer (b)\n",
        "          torch tensor of shape (n_C, 1, 1, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve information from cache\n",
        "    (A_prev, W, b) = cache\n",
        "    # Retrieve dimensions from A_prev's shape\n",
        "    (m, n_C_prev, n_H_prev, n_W_prev) = A_prev.shape\n",
        "    # Retrieve dimensions from W's shape\n",
        "    (n_C_prev, n_C, f, f) = W.shape\n",
        "\n",
        "    # Parameters by default\n",
        "    stride = 1\n",
        "    pad = 0\n",
        "\n",
        "    # Retrieve dimensions from dZ's shape\n",
        "    (m, n_C, n_H, n_W) = dZ.shape\n",
        "\n",
        "    # Initialize dA_prev, dW, db with the correct shapes\n",
        "    dA_prev = torch.zeros(A_prev.shape)\n",
        "    dW = torch.zeros(W.shape)\n",
        "    db = torch.zeros(b.shape)\n",
        "\n",
        "    # Copy A_prev and dA_prev to make changes\n",
        "    A_prev_copy = A_prev.clone()\n",
        "\n",
        "    for i in range(m):                       # loop over the training examples\n",
        "\n",
        "        # select ith training example from A_prev_pad and dA_prev_pad\n",
        "        a_prev_copy = A_prev_copy[i]\n",
        "\n",
        "        for h in range(n_H_prev):                   # loop over vertical axis of the input\n",
        "            for w in range(n_W_prev):               # loop over horizontal axis of the input\n",
        "                for c in range(n_C):                # loop over the channels of the output\n",
        "\n",
        "                    # Find the corners of the current slice\n",
        "                    vert_start = stride * h\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = stride * w\n",
        "                    horiz_end = horiz_start + f\n",
        "\n",
        "                    # Use the corners to define the slice from a_prev_copy\n",
        "                    a_slice = a_prev_copy[:, h, w]\n",
        "\n",
        "                    # Update gradients for the filter's parameters (loop over input channels)\n",
        "                    for x in range(n_C_prev): dW[x,c,:,:] += a_slice[x] * dZ[i, c, vert_start:vert_end, horiz_start:horiz_end]\n",
        "                    #da_prev_pad[:, h, w] += np.sum(W[:, c, :, :] * dZ[i, c, vert_start:vert_end, horiz_start:horiz_end])\n",
        "\n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == (m, n_C_prev, n_H_prev, n_W_prev))\n",
        "\n",
        "    # compute db value\n",
        "    db += m*n_H*n_W\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "9x9ulnHgJ15v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some random values that I'll use for testing\n",
        "in_channels  = random.randint(1, 10)\n",
        "out_channels = random.randint(1, 10)\n",
        "kernel_size  = random.randint(1, 5)\n",
        "m = random.randint(1, 10)\n",
        "n_H_prev = random.randint(5, 10)  # to make it larger than kernel_size\n",
        "n_W_prev = random.randint(5, 10)  # to make it larger than kernel_size\n",
        "\n",
        "# define the standart PyTorch ConvTranspose2d layer\n",
        "model = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
        "\n",
        "# create the same weights and biases to feed them into my custom function\n",
        "torch.nn.init.xavier_uniform_(model.weight)\n",
        "torch.nn.init.uniform_(model.bias)\n",
        "\n",
        "# initialize input with random values\n",
        "A_prev = torch.rand(m, in_channels, n_H_prev, n_W_prev, requires_grad=True)\n",
        "\n",
        "# create the same weights and biases to feed them into my custom function\n",
        "W = model.weight.requires_grad_()\n",
        "b = model.bias[:, None, None, None].requires_grad_()\n",
        "\n",
        "# get results from the custom function (forward path)\n",
        "Z, cache_conv = conv_transpose_forward(A_prev, W, b)\n",
        "# get gradients from the custom function (use ones_like because I have dL/dy like this by default)\n",
        "dA, dW, db = conv_transpose_backward(torch.ones_like(Z), cache_conv)\n",
        "\n",
        "# get results from the standart PyTorch layer\n",
        "model_out = model(A_prev)\n",
        "# to use .backward() method I need to get scalar value\n",
        "T = torch.sum(model_out)\n",
        "# call this method to compute gradients\n",
        "T.backward()\n",
        "\n",
        "# compare the results using mean squared error (MSE) as an appropriate metric\n",
        "dA_error = mean_squared_error(A_prev.grad.detach().numpy().reshape(m, -1), dA.detach().numpy().reshape(m, -1))\n",
        "dW_error = mean_squared_error(model.weight.grad.detach().numpy().reshape(out_channels, -1), dW.detach().numpy().reshape(out_channels, -1))\n",
        "db_error = mean_squared_error(model.bias.grad.detach().numpy().reshape(out_channels, -1), db.detach().numpy().reshape(out_channels, -1))\n",
        "\n",
        "# print out the results\n",
        "#print(\"Gradient of inputs error =\", dA_error)\n",
        "print(\"Gradient of weights error =\", dW_error)\n",
        "print(\"Gradient of biases error =\", db_error, \"\\n\")"
      ],
      "metadata": {
        "id": "j81fX86LQFUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}